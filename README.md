# TinyLlama-with-FastAPI
I  downloaded the TinyLlama model in my local machine and created a FastAPI server that allows for inference using this model.
